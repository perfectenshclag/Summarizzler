# Import Required Modules
import os
import validators
import streamlit as st
from bs4 import BeautifulSoup
import requests
from langchain.prompts import PromptTemplate
from langchain_groq import ChatGroq
from langchain.chains.summarize import load_summarize_chain
from langchain_community.document_loaders import YoutubeLoader
from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings

# Load environment variables
load_dotenv()
os.environ["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY")
os.environ["HF_TOKEN"] = os.getenv("HF_TOKEN")

# Initialize LLM
llm = ChatGroq(model="llama-3.3-70b-specdec")

# Streamlit App Configuration
st.set_page_config(page_title="Summarizzler: The Final Boss!", page_icon="")
st.title(" Summarizzler: The Final Boss")
st.subheader("Summarize or Query URL Content ")

# User Inputs
generic_url = st.text_input("Enter URL", label_visibility="visible")
operation = st.selectbox("Choose Operation:", ["Summarize Content", "Query Extracted Text"])
query = None
if operation == "Query Extracted Text":
    query = st.text_input("Enter your query:", label_visibility="visible")

# Language Selection (only for YouTube links)
selected_language = None
if generic_url and ("youtube.com" in generic_url or "youtu.be" in generic_url):
    try:
        # Extract the video ID
        if "youtube.com" in generic_url:
            video_id = generic_url.split("v=")[-1].split("&")[0]
        elif "youtu.be" in generic_url:
            video_id = generic_url.split("youtu.be/")[-1].split("?")[0]
        else:
            raise ValueError("Invalid YouTube URL format.")

        # Fetch available transcript languages
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        available_languages = []

        for transcript in transcript_list:
            available_languages.append((transcript.language, transcript.language_code))
        for transcript in transcript_list:
            if transcript.is_generated:
                available_languages.append((f"{transcript.language} (Autogenerated)", transcript.language_code))

        # Show the language dropdown
        selected_language = st.selectbox(
            "Select transcript language:",
            options=[lang[0] for lang in available_languages]
        )
    except Exception as e:
        st.warning("Could not fetch transcript languages. Proceeding without selection.")
        st.exception(e)

# Prompt Templates
summary_prompt_template = """
Provide a summary of the following content in 300 words:
Content: {text}
"""
summary_prompt = PromptTemplate(template=summary_prompt_template, input_variables=["text"])

query_prompt_template = """
Answer the following question based on the provided context and if not available in context answer yourself and specify that you've answered:
Question: {query}
Context: {context}
"""
query_prompt = PromptTemplate(template=query_prompt_template, input_variables=["query", "context"])

# Helper Functions
def extract_website_content(url):
    """Extracts text content from a URL."""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        text_elements = soup.find_all(["p", "h1", "h2", "h3", "li", "span"], string=True)
        text = "\n".join(element.get_text(strip=True) for element in text_elements)
        return text if text else "No valid content found on the webpage."
    except Exception as e:
        st.error("Error extracting website content.")
        st.exception(e)
        return ""

def fetch_youtube_transcript(video_url, language_code="en"):
    """Fetches a YouTube transcript in the selected language."""
    try:
        video_id = video_url.split("v=")[-1].split("&")[0] if "youtube.com" in video_url else video_url.split("youtu.be/")[-1].split("?")[0]
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[language_code])
        return " ".join([item["text"] for item in transcript])
    except NoTranscriptFound:
        st.warning("No transcript found for the selected language.")
    except TranscriptsDisabled:
        st.error("Transcripts are disabled for this video.")
    except Exception as e:
        st.error("Error fetching YouTube transcript.")
        st.exception(e)
    return None

def create_vector_db(docs):
    """Creates a FAISS vector database from a list of documents."""
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)
    split_docs = text_splitter.split_documents(docs)
    vector_db = FAISS.from_documents(split_docs, embeddings)
    return vector_db

# Main Workflow
if st.button("Process URL"):
    if not validators.url(generic_url):
        st.error("Please enter a valid URL.")
    else:
        try:
            with st.spinner("Processing..."):
                progress_bar = st.progress(0)
                status_placeholder = st.empty()

                # Step 1: URL content extraction
                progress_bar.progress(20)
                status_placeholder.markdown("**Step 1/4: Extracting website content...**")

                if "youtube.com" in generic_url or "youtu.be" in generic_url:
                    # Map selected language to its code
                    if selected_language:
                        selected_language_code = next(
                            lang[1] for lang in available_languages if lang[0] == selected_language
                        )
                    else:
                        selected_language_code = "en"
                    extracted_text = fetch_youtube_transcript(generic_url, selected_language_code)
                    docs = [Document(page_content=extracted_text)] if extracted_text else []
                else:
                    extracted_text = extract_website_content(generic_url)
                    docs = [Document(page_content=extracted_text)] if extracted_text else []

                if not docs:
                    st.warning("No content available for processing.")
                    st.stop()

                progress_bar.progress(40)
                status_placeholder.markdown("**Step 2/4: Creating vector database...**")

                # Step 2: Create vector database
                vector_db = create_vector_db(docs)

                progress_bar.progress(70)
                if operation == "Summarize Content":
                    status_placeholder.markdown("**Step 3/4: Summarizing content...**")

                    # Summarize content
                    chain = load_summarize_chain(llm, chain_type="stuff", prompt=summary_prompt)
                    output_summary = chain({"input_documents": docs})

                    progress_bar.progress(100)
                    status_placeholder.markdown("**Done! Summary generated.**")

                    st.success("Summary:")
                    st.write(output_summary["output_text"])

                elif operation == "Query Extracted Text" and query:
                    status_placeholder.markdown("**Step 3/4: Querying extracted text...**")

                    # Query content
                    retriever = vector_db.as_retriever()
                    retrieved_docs = retriever.get_relevant_documents(query)

                    if not retrieved_docs:
                        st.warning("No relevant documents found for the query.")
                        st.stop()

                    progress_bar.progress(90)
                    status_placeholder.markdown("**Step 4/4: Generating query result...**")

                    # Combine retrieved content into a single context
                    combined_context = "\n".join(doc.page_content for doc in retrieved_docs)

                    # Use the custom query prompt
                    query_input = query_prompt.format(query=query, context=combined_context)
                    result = llm.predict(query_input)

                    progress_bar.progress(100)
                    status_placeholder.markdown("**Done! Query result generated.**")

                    st.success("Query Result:")
                    st.write(result)
        except Exception as e:
            st.error("An unexpected error occurred.")
            st.exception(e)
